<!doctype html>
<html lang="en">

<head>
    <title>Projects - tmralmeida</title>
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>


    <link rel="canonical" href="https://getbootstrap.com/docs/4.3/examples/cover/">

    <!-- Bootstrap core CSS -->
     <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
        integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="styles/styles.css" rel="stylesheet">
    <link href="styles/projects.css" rel="stylesheet">
    

    <script src="scripts/toggle.js"> </script>
</head>

<body class="text-center">
    <div class="cover-container d-flex w-100 h-100 p-3 mx-auto flex-column">
        <header class="masthead mb-auto">
            <div class="inner">
                <h3 class="masthead-brand">tmralmeida</h3>
                <nav class="nav nav-masthead justify-content-center">
                    <a class="nav-link" href="index.html">Home</a>
                    <a class="nav-link active" href="projects.html">Projects</a>
                    <a class="nav-link" href="publications.html">Publications</a>
                    <a class="nav-link" href="resume.html">Resume</a>
                    <a class="nav-link" href="contacts.html">Contacts</a>
                </nav>
            </div>
        </header>

        <main role="main" class="inner cover">


            <div class="card">
              <h2 class="card-header">bag-of-models</h2>
              <div class="card-body">
                  <h5 class="card-title">Mar 2020 - now</h5>
                  <div class="progress">
                      <div class="progress-bar" role="progressbar" style="width: 100%;" aria-valuenow="25" aria-valuemin="0" aria-valuemax="100">Finished</div>
                    </div>
                  <p class="card-text">This is a guide for Deep Learning practitioners. It covers Tensorflow and Pytorch
                  techniques to train the best-known models for Image Classification and...</p>
                <button id="read_more_bag" type="button" class="btn btn-primary" data-toggle="modal" data-target="#modal-bag">
                    Continue reading
                </button>
                <div class="modal fade" id="modal-bag" tabindex="-1" role="dialog" aria-labelledby="bag-modal-title" aria-hidden="true">
                  <div class="modal-dialog modal-lg" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <h4 class="modal-title" id="bag-modal-title">bag-of-models Project</h4>
                        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                          <span aria-hidden="true">&times;</span>
                        </button>
                      </div>
                      <div class="modal-body">
                        
                        <p class="describe-projects">
                          This is a guide for Deep Learning practitioners. It covers Tensorflow and Pytorch techniques to train the best-known models for Image Classification and Object Detection fields. At the beginning
                          of my journey of learning this topic in practice, the most difficult thing for me was filtering out all the information, because every practicioner has one repository
                          and it seems that they have results but their code is too complex for a beginner. Therefore, I started with a <a href="https://www.coursera.org/specializations/tensorflow-in-practice"
                          class="tooltip-test">Tensorflow Specialization</a> and as I was learning, I was doing my test cases for myself. In my opinion, the easiest way to start is with Image Classification because 
                          it does not resort as much as effort as the other fields. The effort here is important, because it is an effort related to the complexity of conceiving the model in practice, so less
                          effort means a more understandable and easier code. Thus, I started to download one dataset (CINIC10), then I tried to replicate the models training, which I was studying 
                          through the respective papers (I went from AlexNet to MobileNet). The code is not the most efficient one but it was done by a beginner so I hope that it is clear enough.
                        </p class="describe-projects">
                        <p class="describe-projects">
                          After Image Classification, I wanted to study Object Detection, which seems a trendy Computer Vision task but it was difficult to assimilate all the little tricks behind each 
                          choice of the authors of the most well-known architectures. At the same time, in my work the opportunity of working also in Object Detection arose. So, it was a win-win situation.
                          First, I attend the <a href="https://deeplizard.com/learn/video/v5cngxo4mIg" class="tooltip-test">deeplizard</a> course about Pytorch because I wanted to know all the decent possibilities
                          I had in terms of Deep Learning frameworks. Hence, Pytorch was used to study those architectures (from Faster R-CNN up to YOLOv4).
                        </p>
                        <p class="describe-projects">
                          Now, you can decide which of the branches of this project you want to check:
                       
                          <div class="btn-group" role="group">
                            <button id="btnGroupDrop1" type="button" class="btn btn-secondary dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                              Fields
                            </button>
                            <div class="dropdown-menu" aria-labelledby="btnGroupDrop1">
                              <a class="dropdown-item" onclick="toggle_modals('#modal-bag', '#modal-classification')">Image Classification</a>
                              <a class="dropdown-item" onclick="toggle_modals('#modal-bag', '#modal-detection')">Object Detection</a>
                            </div>
                          </div>
                      </p>
                      </div>
                      <div class="modal-footer">
                        <button id="prev_bag" type="button" class="btn btn-primary" onclick="toggle_modals('#modal-bag', '#modal-faster')">Previous</button>
                        <button type="button" class="btn btn-primary" disabled>Next</button>
                        <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                      </div>
                    </div>
                  </div>
                </div>

              </div>
            </div>

            <!-- Bag of models fields -->
            <!-- Classification -->
            <div class="modal fade" id="modal-classification" tabindex="-1" role="dialog" aria-labelledby="classification-modal-title" aria-hidden="true">
              <div class="modal-dialog modal-lg" role="document">
                <div class="modal-content">
                  <div class="modal-header">
                    <h4 class="modal-title" id="classification-modal-title">Image Classification</h4>
                    <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                      <span aria-hidden="true">&times;</span>
                    </button>
                  </div>
                  <div class="modal-body">
                    <p class="describe-projects">
                      First of all, for data loading I used in every model the <a href="https://www.tensorflow.org/guide/data" class="tooltip-test">tf.data</a> module. It allows to create
                      a full pipeline that aggregates: loading from disk, data augmentation, and batch formation. I did not go too deep into augmentation because the objective at this point
                      would be to practice the models creation and try to understand the various ways of doing it by using Tensorflow2.0. In my opinion, there are three global ways to deploy a 
                      Tensorflow model, whose usage depends on the architecture's layout. If the model is straightforward (the easiest ones) we can use the 
                      <a href="https://www.tensorflow.org/api_docs/python/tf/keras/Sequential" class="tooltip-test">Sequential API</a>; 
                      on the other hand, if the model resorts layers concatenation and "parallel operations" (more complex models), we should use the 
                      <a href="https://www.tensorflow.org/guide/keras/functional" class="tooltip-test"> Functional API</a>; finally, if we want a fully-customizable foward 
                      propagation we can use <a href="https://www.tensorflow.org/guide/keras/custom_layers_and_models" class="tooltip-test">Model subclassing</a>. 
                    </p>
                    <p class="describe-projects">
                      During this study, I just used the Sequential API for the easiest models and the Functional API for the more complex ones. Therefore, the first three models - AlexNet, ZFNet 
                      and VGG16 - were created under the Sequential API due to their simple design. The remaining models - ResNet18, GoogLeNet, Xception and MobileNet - were designed through the Functional
                      API.
                    </p>
                    <p class="describe-projects">
                      Theoretically, it is important to highlight some key points in the history of Convolutional Neural Networks for Image Classification, which are now used or have an influence on
                      the most modern architectures:

                      <ul class="list-group list-group-flush" style="color:black;font-weight:300;">
                        <li class="list-group-item"><b>AlexNet</b> is the first Convolutional Neural Network that obtained a quite important result in the ImageNet challenge;</li>
                        <li class="list-group-item"><b>ZFNet</b> showed how it would be possible to improve the network's layout by visualizing what is going on inside of it;</li>
                        <li class="list-group-item"><b>VGG16</b> showed that deeper convolutional neural networks can be more accurate than shallower networks; </li>
                        <li class="list-group-item">The more layers a neuronal network has, the harder it is to train. Thus, <b>ResNet</b> showed how it is possible to train deep neural networks 
                          in a simpler fashion by applying residual blocks with skip connections. The image below demonstrates this design choice (from <a href="https://d2l.ai/chapter_convolutional-modern/resnet.html"
                          class="tooltip-test">Dive Into Deep Learning</a>); the left image represents the original residual block and the right image illustrates the residual block with 
                          the respective skip connection - trick that makes the network training easier.</li>
                        </ul> 
                      <img class="img-responsive" src="images/projects/bag-of-models/classification/resblock.png" width="60%" alt="resblock and skip connections">
                    </p>
                    
                    <p class="describe-projects">
                      <b>You can check all notebooks at </b><a href="https://github.com/tmralmeida/bag-of-models/tree/master/CNNs/1-Image_Classification" class="tooltip-test">tmralmeida</a>.
                    </p>
                    
                   
                  </div>
                  <div class="modal-footer">
                    <button type="button" class="btn btn-primary" onclick="toggle_modals('#modal-classification', '#modal-bag')">Previous</button>
                    <button type="button" class="btn btn-primary" onclick="toggle_modals('#modal-classification', '#modal-detection')">Next</button>
                    <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                  </div>
                </div>
              </div>
            </div>

            <!-- Detection -->
            <div class="modal fade" id="modal-detection" tabindex="-1" role="dialog" aria-labelledby="detection-modal-title" aria-hidden="true">
              <div class="modal-dialog modal-lg" role="document">
                <div class="modal-content">
                  <div class="modal-header">
                    <h4 class="modal-title" id="detection-modal-title">Object Detection</h4>
                    <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                      <span aria-hidden="true">&times;</span>
                    </button>
                  </div>
                  <div class="modal-body">
                    <p class="describe-projects">
                      As mentioned before, the study of object detection in this project came at the same time as a task that I had to complete professionally. At that time, I was studying object detection architectures in 
                      different road environments for autonomous driving. Therefore, after studying several available datasets, the one that proved to be most representative in terms of the quality/diversity of information ratio was 
                      the <a href="https://bair.berkeley.edu/blog/2018/05/30/bdd/" class="tooltip-test">BDD100K</a>. This dataset is composed of several different types of annotations, and the one we worked on 
                      (road object bounding boxes) is divided into 10 different classes: bus, light, sign, person, bike, truck, motor, car, train, and rider. 
                    </p>
                    <p class="describe-projects">
                      After choosing the dataset, I also study the state-of-the-art of Object Detection arquitectures. From here, two major types of architectures for the Object Detection task arose: proposals networks 
                      and single shot methods. The former is represented by Faster R-CNN, which I had already used in another project. The latter is composed of SSD (Single Shot Detection) and all versions of YOLO. Hence,
                      these are the architectures that I would use to perform this study. Since I had deadlines to present results in my work, the models that I used here are not originally made by me, but based on works of
                      other authors as I will reference later.
                    </p>

                    <!-- Faster -->
                    <p class="describe-projects">
                      <h5 style="color:black">1. Faster R-CNN</h5>
                          <p class="describe-projects">
                            Faster RCNN is one of the most widely used deep learning models for object detection. Although, its high-latency comparing to single-shot methods, Faster RCNN is performant 
                            detecting both small and large objects. The authors of this DL architecture divide the overall architecture into 2 modules, however, it is fairer to divide it into 3 modules: 
                            feature maps extractor, RPN (Region Proposals Network) and Fast R-CNN detector.
                            The former is composed of a traditional classification architecture, which is responsible for producing feature maps. In our approach we choose a MobileNetV2 to perform this 
                            task due to its low-latency. After that, a small network slides over the feature maps predicting multiple possible proposals for each of its cells. This small network returns a 
                            lower-dimensional feature, which is then fed to two 1 * 1 convolutional layers. These layers yield the probability of a proposal bounding a target, and the encoded coordinates of
                             each proposal, respectively. Finally, the features that correspond to objects pass through an ROI pooling layer that crops and rescales each feature. During inference, the non-maximum 
                             suppression (NMS) algorithm is computed to filter out the best-located bounding boxes.
                          </p>
                          <p class="describe-projects">
                            The work that we developed here in terms of training and model creation was based on the <a href="https://github.com/pytorch/vision/tree/master/torchvision" class="tooltip-test">torchvision 
                            module</a> of Pytorch framework.
                          </p>
                          <p class="describe-projects">
                            The numeric results for the validation set, based on <a href="http://cocodataset.org/#detection-eval" class="tooltip-test">COCO metrics</a> are represented in the table below. 
                          </p>
                      
                          <table class="tg">
                            <thead>
                              <tr>
                                <th class="tg-0pky"></th>
                                <th class="tg-c3ow">IoU Thresholds</th>
                                <th class="tg-c3ow">Scales</th>
                                <th class="tg-c3ow">maxDets</th>
                                <th class="tg-c3ow">AP/AR values</th>
                              </tr>
                            </thead>
                            <tbody>
                              <tr>
                                <td class="tg-9wq8" rowspan="6">AP</td>
                                <td class="tg-c3ow">[0.50 : 0.05 : 0.95]</td>
                                <td class="tg-c3ow">all</td>
                                <td class="tg-c3ow">100</td>
                                <td class="tg-c3ow">0.202</td>
                              </tr>
                              <tr>
                                <td class="tg-c3ow">0.50</td>
                                <td class="tg-c3ow">all</td>
                                <td class="tg-c3ow">100</td>
                                <td class="tg-c3ow">0.409</td>
                              </tr>
                              <tr>
                                <td class="tg-c3ow">0.75</td>
                                <td class="tg-c3ow">all</td>
                                <td class="tg-c3ow">100</td>
                                <td class="tg-c3ow">0.175</td>
                              </tr>
                              <tr>
                                <td class="tg-c3ow">0.95</td>
                                <td class="tg-c3ow">small</td>
                                <td class="tg-c3ow">100</td>
                                <td class="tg-c3ow">0.050<br></td>
                              </tr>
                              <tr>
                                <td class="tg-c3ow">[0.50 : 0.05 : 0.95]</td>
                                <td class="tg-c3ow">medium</td>
                                <td class="tg-c3ow">100</td>
                                <td class="tg-c3ow">0.243<br></td>
                              </tr>
                              <tr>
                                <td class="tg-c3ow">[0.50 : 0.05 : 0.95]<br></td>
                                <td class="tg-c3ow">large</td>
                                <td class="tg-c3ow">100</td>
                                <td class="tg-c3ow">0.432<br></td>
                              </tr>
                              <tr>
                                <td class="tg-lboi" rowspan="6">AR</td>
                                <td class="tg-c3ow">[0.50 : 0.05 : 0.95]</td>
                                <td class="tg-c3ow">all</td>
                                <td class="tg-c3ow">1</td>
                                <td class="tg-c3ow">0.158</td>
                              </tr>
                              <tr>
                                <td class="tg-c3ow">[0.50 : 0.05 : 0.95]</td>
                                <td class="tg-c3ow">all</td>
                                <td class="tg-c3ow">10</td>
                                <td class="tg-c3ow">0.277</td>
                              </tr>
                              <tr>
                                <td class="tg-c3ow">[0.50 : 0.05 : 0.95]</td>
                                <td class="tg-c3ow">all</td>
                                <td class="tg-c3ow">100</td>
                                <td class="tg-c3ow">0.290</td>
                              </tr>
                              <tr>
                                <td class="tg-c3ow">[0.50 : 0.05 : 0.95]</td>
                                <td class="tg-c3ow">small</td>
                                <td class="tg-c3ow">100</td>
                                <td class="tg-c3ow">0.116</td>
                              </tr>
                              <tr>
                                <td class="tg-c3ow">[0.50 : 0.05 : 0.95]</td>
                                <td class="tg-c3ow">medium</td>
                                <td class="tg-c3ow">100</td>
                                <td class="tg-c3ow">0.355<br></td>
                              </tr>
                              <tr>
                                <td class="tg-c3ow">[0.50 : 0.05 : 0.95]</td>
                                <td class="tg-c3ow">large</td>
                                <td class="tg-c3ow">100</td>
                                <td class="tg-c3ow">0.519<br></td>
                              </tr>
                            </tbody>
                          </table>
                        
                          <p class="describe-projects">
                              Finally, I release videos that demonstrate part of the qualitative results of the trained model in frames acquired in Aveiro roads. One example of those 
                              videos is shown below.
                          </p>                            
                            <iframe width="100%" height="315" src="https://www.youtube.com/embed/o7KSFDoEfW0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                    </p>

                    <!-- SSD -->
                    <p class="describe-projects">
                      <h5 style="color:black">2. SSD512</h5>
                          <p class="describe-projects">
                            Single shot models can process the input faster due to the respective tasks - localization and classification - be done in a single forward fashion. 
                            Here, SSD is presented as well as its results in the validation set of the dataset used in this work. This architecture is characterized by its base 
                            network (or backbone), the usage of multi-scaled feature maps for the detection task, and the respective convolutional predictors. MobileNetV2 was used 
                            to perform the perception of the image features and then was truncated before the classification layers. Hence, some of the final layers of MobileNet 
                            and additional feature layers allow multiple scales predictions. Each of these extra layers can produce a fixed set of detection predictions 
                            using a set of convolutional filters. Finally, the output of the model is the score for a category and the location of the box that bounds the target object.
                          </p>
                          <p class="describe-projects">
                            This work, in terms of code, is based on the one of <a href="https://github.com/qfgaohao/pytorch-ssd" class="tooltip-test">qfgaohao</a>. However, here I did some adaptations
                            to increase the performance of the model. One of them is the 512*512 input size.
                          </p>
                          <p class="describe-projects">
                            Finally, the numeric results for the BDD100K validation set are represented in the table below.
                          </p>
                      
                          <table class="tg">
                            <thead>
                              <tr>
                                <th class="tg-0pky"></th>
                                <th class="tg-c3ow">IoU Thresholds</th>
                                <th class="tg-c3ow">Scales</th>
                                <th class="tg-c3ow">maxDets</th>
                                <th class="tg-c3ow">AP/AR values</th>
                              </tr>
                            </thead>
                            <tbody>
                              <tr>
                                <td class="tg-9wq8" rowspan="6">AP</td>
                                <td class="tg-c3ow">[0.50 : 0.05 : 0.95]</td>
                                <td class="tg-c3ow">all</td>
                                <td class="tg-c3ow">100</td>
                                <td class="tg-c3ow">0.083</td>
                              </tr>
                              <tr>
                                <td class="tg-c3ow">0.50</td>
                                <td class="tg-c3ow">all</td>
                                <td class="tg-c3ow">100</td>
                                <td class="tg-c3ow">0.131</td>
                              </tr>
                              <tr>
                                <td class="tg-c3ow">0.75</td>
                                <td class="tg-c3ow">all</td>
                                <td class="tg-c3ow">100</td>
                                <td class="tg-c3ow">0.085</td>
                              </tr>
                              <tr>
                                <td class="tg-c3ow">0.95</td>
                                <td class="tg-c3ow">small</td>
                                <td class="tg-c3ow">100</td>
                                <td class="tg-c3ow">0.002<br></td>
                              </tr>
                              <tr>
                                <td class="tg-c3ow">[0.50 : 0.05 : 0.95]</td>
                                <td class="tg-c3ow">medium</td>
                                <td class="tg-c3ow">100</td>
                                <td class="tg-c3ow">0.044<br></td>
                              </tr>
                              <tr>
                                <td class="tg-c3ow">[0.50 : 0.05 : 0.95]<br></td>
                                <td class="tg-c3ow">large</td>
                                <td class="tg-c3ow">100</td>
                                <td class="tg-c3ow">0.293<br></td>
                              </tr>
                              <tr>
                                <td class="tg-lboi" rowspan="6">AR</td>
                                <td class="tg-c3ow">[0.50 : 0.05 : 0.95]</td>
                                <td class="tg-c3ow">all</td>
                                <td class="tg-c3ow">1</td>
                                <td class="tg-c3ow">0.068</td>
                              </tr>
                              <tr>
                                <td class="tg-c3ow">[0.50 : 0.05 : 0.95]</td>
                                <td class="tg-c3ow">all</td>
                                <td class="tg-c3ow">10</td>
                                <td class="tg-c3ow">0.093</td>
                              </tr>
                              <tr>
                                <td class="tg-c3ow">[0.50 : 0.05 : 0.95]</td>
                                <td class="tg-c3ow">all</td>
                                <td class="tg-c3ow">100</td>
                                <td class="tg-c3ow">0.093</td>
                              </tr>
                              <tr>
                                <td class="tg-c3ow">[0.50 : 0.05 : 0.95]</td>
                                <td class="tg-c3ow">small</td>
                                <td class="tg-c3ow">100</td>
                                <td class="tg-c3ow">0.005</td>
                              </tr>
                              <tr>
                                <td class="tg-c3ow">[0.50 : 0.05 : 0.95]</td>
                                <td class="tg-c3ow">medium</td>
                                <td class="tg-c3ow">100</td>
                                <td class="tg-c3ow">0.052<br></td>
                              </tr>
                              <tr>
                                <td class="tg-c3ow">[0.50 : 0.05 : 0.95]</td>
                                <td class="tg-c3ow">large</td>
                                <td class="tg-c3ow">100</td>
                                <td class="tg-c3ow">0.334<br></td>
                              </tr>
                            </tbody>
                          </table>
                        
                          <p class="describe-projects">
                            Although a huge difference between the numerical results for the validation set between the two architectures presented so far, this model is also 
                            performant on Aveiro roads. Please, check the video below.
                          </p>    

                          <iframe width="100%" height="315" src="https://www.youtube.com/embed/DGnp-dfRaXU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                   
                    </p>

                    <!-- YOLOV4 -->
                    <p class="describe-projects">
                      <h5 style="color:black">3. YOLOV4</h5>
                          <p class="describe-projects">
                            All YOLO architectures are also single-shot methods, and that is why they achieve high-speed predictions. The authors have been presenting several evolutions, 
                            which is proved by the amount of YOLO versions that exist - 4 until the writing date of this post (YOLO, YOLOv2, YOLOv3, and YOLOv4). This architecture 
                            has always shown low-latency and, therefore, what has been the focus along the various versions is the localization performance.

                            YOLOv4 is composed of a Cross Stage Partial (CSP) Darknet53 with an SPP module, a path-aggregation net (PANet), and a 
                            YOLOv3 head. CSP networks have similar basis and purposes to a DenseNet. Therefore, this type of architectures enhances the features reuse by reducing the 
                            amount of repeated gradient information observed in a DenseNet. To do so, it divides the base feature map, then a part of the channels  passes through a 
                            partial dense block and the other part undergoes to the final partial transition layer. After activation maps production, the only difference between YOLOv3 
                            and YOLOv4 in terms of architecture's layout is the global features concatenation. Instead of the FPN technique, a custom PANet approach is performed.
                            PANet is simply an enhanced version of FPN; after the FPN's block composed of a top-down pathway with lateral connections, PANet also propagates low-level 
                            features through a bottom-up path augmentation block. This block allows the addition (concatenation for YOLOv4) of the FPN resulting features with the output 
                            of those feature maps with 3*3 convolutions, which yields an even better understanding of the low-level features.
                          </p>
                          <p class="describe-projects">
                            This work, in terms of code, is based on the one of <a href="https://github.com/ultralytics/yolov3" class="tooltip-test">Ultralytics</a> with some changes to allow the usage
                            of the Ignite framework.
                          </p>
                          <p class="describe-projects">
                            I also deployed this model on a Nvidia Jetson AGX Xavier device and you can check the result in the video below and the demo code is available in <a href="https://github.com/tmralmeida/tensorrt-yolov4" 
                            class="tooltip-test">tmralmeida</a>.
                          </p>
                          

                          <iframe width="100%" height="315" src="https://www.youtube.com/embed/63p3GyR8JPw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
          
                    </p>

                    <p class="describe-projects">
                      <b>You can check the repository at </b><a href="https://github.com/tmralmeida/bag-of-models/tree/master/CNNs/2-Object_Detection/scripts" class="tooltip-test">tmralmeida</a>.
                    </p>

                  </div>
                  <div class="modal-footer">
                    <button type="button" class="btn btn-primary" onclick="toggle_modals('#modal-detection', '#modal-classification')">Previous</button>
                    <button type="button" class="btn btn-primary" disabled>Next</button>
                    <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                  </div>
                </div>
              </div>
            </div>









            <!-- DATA MATRIX PROJECT -->
            <div class="card">
              <h2 class="card-header">faster-rcnn-data-matrix</h2>
              <div class="card-body">
                  <h5 class="card-title">Mar 2020</h5>
                   <div class="progress">
                      <div class="progress-bar" role="progressbar" style="width: 100%;" aria-valuenow="100" aria-valuemin="0" aria-valuemax="100">Finished</div>
                    </div>
                  <p class="card-text">This work presents an implementation of a Faster R-CNN model to detect Data Matrix. This architecture
                  demonstrated quite accurate and consistent results by...</p>

                <button type="button" class="btn btn-primary" data-toggle="modal" data-target="#modal-faster">
                    Continue reading
                </button>
                <!-- Modal -->
                <div class="modal fade" id="modal-faster" tabindex="-1" role="dialog" aria-labelledby="faster-modal-title" aria-hidden="true">
                  <div class="modal-dialog modal-lg" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <h4 class="modal-title" id="faster-modal-title">faster-rcnn-data-matrix Project</h4>
                        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                          <span aria-hidden="true">&times;</span>
                        </button>
                      </div>
                      <div class="modal-body">
                          <p class="describe-projects">
                              This work presents an implementation of a Faster R-CNN model to detect Data Matrix. This architecture
                              demonstrated quite accurate and consistent results by detecting almost all landmarks throughout the test
                              set.
                          </p>
                          <p class="describe-projects">
                              It arose during my research work at University of Aveiro, Portugal. In this project, I went through
                              every step of training a deep neural network: data collection (images of this type of landmarks in different environments);
                              data labeling through the <a href="https://labelbox.com/" class="tooltip-test">Labelbox app</a>; then, the Faster R-CNN model 
                              was trained and evaluated through the <a href="https://github.com/facebookresearch/detectron2" class="tooltip-test">
                              Detectron2 platform</a>, which is a research platform that contains several state-of-the-art models such as Faster R-CNN, Mask R-CNN, 
                              RetinaNet, and DensePose ready to use. 
                          </p>
                          <p class="describe-projects">
                              <b>Advice:</b> For those who don't have much time to design the architecture, this kind of platforms is totally worth it.
                          </p>
                          <p class="describe-projects">
                            <h5 style="color:black">1. Dataset creation</h5>
                              <p class="describe-projects">
                                The dataset is one of the most important pieces of the overall Machine Learning solution, since each decision of the model is based on 
                                a previous training, which is performed on that data. Therefore, if the training procedure has been compromised, then the inference quality of the model will be worse.
                                Thus, in this stage of the work, we labeled correctly 156 training frames and 224 test images. This distribution of training/test sets
                                is not either the most common one or the most correct one. However, the number of class objects to detect is just one, and, although it
                                is a small patch of the image, it is a pretty distinguishable object from the rest of the image. So, the training set is equally composed of
                                two different environments: a common laboratory room with several objects spread around and a workshop with machinery. These choices allow to
                                obtain a more representative dataset. Regarding the test set, this is also equally distributed in two different enviroments: a hallway and a different
                                part of the workshop used in the training set.
                              </p>
                              <p class="describe-projects"> 
                                The two images below are 2 samples used in the training set. The left image represents an environment associated to a manufacturing facility and the 
                                right image represents a visually cluttered environment (many different objects) in a laboratory room.
                                <div class="row">
                                  <div class="col">
                                    <img class="img-responsive" src="images/projects/faster-data-matrix/train1.png" style="width: 100%;" alt="image 1 from train set">
                                  </div>
                                  <div class="col">
                                    <img class="img-responsive" src="images/projects/faster-data-matrix/train2.png" style="width: 100%;" alt="image 2 from train set">
                                  </div>
                                </div>
                              </p>
                              <p class="describe-projects">
                               Regarding the test set, two images are shown as examples: from a more visually neat environment (left image) to a more filled and cluttered one (
                                right image). 
                                <div class="row">
                                  <div class="col">
                                    <img class="img-responsive" src="images/projects/faster-data-matrix/test2.png" style="width: 100%" alt="image 1 from test set">
                                  </div>
                                  <div class="col">
                                    <img class="img-responsive" src="images/projects/faster-data-matrix/test1.png" style="width: 100%;" alt="image 2 from test set">
                                  </div>
                                  
                                </div>
                              </p>
                            </p>

                          <p class="describe-projects">
                            <h5 style="color:black">2. Faster R-CNN Training</h5>
                            <p class="describe-projects">
                              We decided to use this architecture because this type of deep neural networks is very performant (in comparison to other object detection architectures) when the 
                              objective is to detect small patches of the image. 
                              Moreover, the system where this neural network would be used (an Automated Guided Vehicle) does not move at high speeds, so the high-latency disadvantage of a proposal 
                              network would not be a problem in this application. 
                              It is also worth mentioning that when you are at the phase of choosing which Machine Learning approach to use, you have to take into account the practical
                              application where you are working at (Deep Learning is sometimes overkill for some applications, Machine Learning is much more than just Deep Learning).
                            </p>
                            <p class="describe-projects">
                              The training procedure of a deep neural network can be divided into 3 main steps: data loading, forward propagation and back propagation. The first step in this work 
                              implied to register our dataset in the dataset catalog of Detectron2. This is no more than a function that translates our dataset in a dictionary with certain fields.
                              You can check all these steps in my <a href="https://github.com/tmralmeida/faster-rcnn-data-matrix/blob/master/faster-rcnn-data-matrix.ipynb" class="tooltip-test">notebook</a>.
                              Finally, the second and third steps, Detectron2 makes everything by us, we just need to know how to use their API and choose some hyperparameters such as: batch size, 
                              learning rate, and the number of iterations.  
                            </p>
                          </p>
                          <p class="describe-projects">
                            <h5 style="color:black">3. Faster R-CNN Evaluation</h5>
                            <p class="describe-projects">
                              The evaluation of the model was also performed through the Detectron2 API. To do so, we evaluate our model trough the <a href="http://cocodataset.org/#detection-eval" 
                              class="tooltip-test">COCO metrics</a> (the figure below shows our results).
                            </p>
                            
                            <img class="img-responsive" src="images/projects/faster-data-matrix/results.png" style="width: 60%" alt="results test set">
                            
                            <p class="describe-projects">
                              The most important overall result is 0.876 for AP@0.5. Why? Because 0.5 is a fair value for the IoU threshold, the scales are all and the number maximum of 
                              detections is 100 (a suitable value to match the reality). Moreover, the recall is higher than the precision, implying that the number of false positives is 
                              higher than the number of false negatives. This means that the model detects almost all the Data Matrix landmarks, but also detects some other objects that are not. In our
                              system this is preferable since we use a Data Matrix decoder in a further step. So, if the detected object is not a Data Matrix, the decoder would return nothing. 
                              Comparing to the Data Matrix detection provided by <a href="https://pypi.org/project/pylibdmtx/" class="tooltip-test">libdmtx Python library</a>, only 45% of the test 
                              set frames were accurately processed by this classical algorithm, being 40 times slower than the model that we trained in this project. 
                            </p>
                            <p class="describe-projects">
                              Finally, we show a video that demonstrate part of the qualitative results of the test set. The results shown here are not at the normal speed due to the video size 
                              (this is 1fps and our model can achieve 7.4fps).  
                            </p>                            
                              <iframe width="100%" height="315" src="https://www.youtube.com/embed/41Ul3dbbNDw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                         
                            <p class="describe-projects">
                              <b>You can check the repository at </b><a href="https://github.com/tmralmeida/faster-rcnn-data-matrix" class="tooltip-test">tmralmeida</a>.
                            </p>
                          </p>
                        </div>
                      <div class="modal-footer">
                        <button type="button" class="btn btn-primary" disabled>Previous</button>
                        <button type="button" class="btn btn-primary" onclick="toggle_modals('#modal-faster,#modal-bag')">Next</button>
                        <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            

        </main>

        <footer class="mastfoot mt-auto">
            <div class="inner">
                <p>last update: 02/06/2020</p>
            </div>
        </footer>
    </div>


</body>

</html>