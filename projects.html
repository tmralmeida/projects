<!doctype html>
<html lang="en">

<head>
    <title>Projects - Tiago R. de Almeida</title>
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


    <link rel="canonical" href="https://getbootstrap.com/docs/4.3/examples/cover/">

    <!-- Bootstrap core CSS -->
     <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
        integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="styles/styles.css" rel="stylesheet">
    <link href="styles/projects.css" rel="stylesheet">
    

    <script src="scripts/toggle.js"> </script>
</head>

<body class="text-center">
    <div class="cover-container d-flex w-100 h-100 p-3 mx-auto flex-column">
        <header class="masthead mb-auto">
            <div class="inner">
                <h3 class="masthead-brand">Tiago R. de Almeida</h3>
                <nav class="nav nav-masthead justify-content-center">
                    <a class="nav-link" href="index.html">Home</a>
                    <a class="nav-link active" href="projects.html">Projects</a>
                    <a class="nav-link" href="publications.html">Publications</a>
                    <a class="nav-link" href="resume.html">Resume</a>
                    <a class="nav-link" href="contacts.html">Contacts</a>
                </nav>
            </div>
        </header>

        <main role="main" class="inner cover">


          <!-- PhD -->
          <div class="card">
            
            <h2 class="card-header">PhD: "Learning to Understand and Predict Heterogeneous Trajectory Data"</h2>
            <div class="card-body">
                <p class="card-text"> Robots and intelligent systems navigating dynamic environments must predict the intentions of surrounding agents to ensure safe and efficient operation. 
                  Trajectory prediction, which captures these intentions through motion patterns... </p>
              <button id="read_more_rl" type="button" class="btn btn-primary" data-toggle="modal" data-target="#modal-phd">
                  Continue reading
              </button>
              <div class="modal fade" id="modal-phd" tabindex="-1" role="dialog" aria-labelledby="bag-phd-title" aria-hidden="true">
                <div class="modal-dialog modal-lg" role="document">
                  <div class="modal-content">
                    <div class="modal-header">
                      <h4 class="modal-title" id="shower-modal-title">PhD: "Learning to Understand and Predict Heterogeneous Trajectory Data"</h4>
                      <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                        <span aria-hidden="true">&times;</span>
                      </button>
                    </div>
                    <div class="modal-body">
                      
                      <p class="describe-projects">
                        <h5 style="color:black">1. Motivation and Thesis Overview</h5>
                        <p class="describe-projects">
                          The thesis can be found <a href="https://oru.diva-portal.org/smash/record.jsf?pid=diva2%3A1985105&dswid=2862 class=" class="tooltip-test">here</a>.
                        Robots and intelligent systems navigating dynamic environments must predict the intentions of surrounding agents to ensure safe and efficient operation. 
                        Trajectory prediction, which captures these intentions through motion patterns, is particularly challenging due to the highly diverse context of motion, expressed
                        in agent-specific and environment cues. Despite this, many current prediction methods focus on homogeneous datasets, limiting their applicability in heterogeneous real-world scenarios.
                        This thesis addresses motion heterogeneity by introducing the concept of trajectory classes, which group data samples based on shared characteristics, either observable (e.g., agent type, activity) or learned from the data itself.
                        These classes are priors for effectively modeling diverse behaviors. However, existing literature lacks both suitable datasets and frameworks to exploit trajectory classes.
                        To bridge this gap, the thesis introduces THÖR-MAGNI, a large-scale
                        dataset capturing diverse human activities in industrial robotics environments.
                        Using this dataset and a real-world autonomous driving dataset,the thesis proposes deep learning-based prediction models conditioned on observable classes,
                        highlighting trade-offs between scalability, data efficiency, and performance
                        under class imbalance. While observable classes enhance prediction accuracy,
                        their static nature can limit their representation power, given the dynamic
                        behavior of humans. To overcome the limitations of static class labels, THÖR-MAGNI <i>Act</i> introduces fine-grained, frame-level action annotations. Actions
                        augmenting the state representation of trajectories have been shown to enhance
                        predictions when integrated via direct conditioning or multi-task learning.
                        Alternatively, data-driven classes group trajectories based on motion patterns learned from the data. We propose a novel Self-Conditioned GAN to learn
                        trajectory clusters aligned with generative modeling objectives. Our method
                        enhances prediction accuracy for underrepresented behaviors and is integrated
                        into a multi-stage prediction framework that explicitly conditions predictions
                        on trajectory clusters, yielding probabilistically informed forecasts.
                        In summary, this thesis contributes datasets, predictive models, and generative frameworks for understanding and predicting heterogeneous trajectory data.

                        <p class="describe-projects">
                        <h5 style="color:black">2. Notation and Terminology</h5>
                        <p class="describe-projects">
                            <ul>
                              <li>Vectors are denoted by bold lowercase letters, e.g., <b>v</b>.</li>
                              <li>Matrices are denoted by bold uppercase letters, e.g., <b>M</b>.</li>
                              <li>We refer to the Frobenius or Euclidean norm of a vector v as ||<b>v</b>||<sub>F</sub>.</li>
                              <li>We denote the prior probability of an event by <i>P</i>(.) and the probability of an event conditioned on a different event as <i>P</i>(.|.).</li>
                              <li>Subscripts are frequently used to indicate relationships between different mathematical objects; for instance, <b>Y</b><sub><b>S</b></sub> denotes a matrix <b>Y</b> that is associated with the matrix <b>S</b> in some way.</li>
                              <li>We use the subscript <i>t</i> to indicate time steps, e.g. <i>s</i><sub><i>t</i></sub> and <i>s</i><sub><i>t+1</i></sub>.</li>
                            </ul> 
                          </p>
                          <p class="describe-projects">
                            The term <i>training or validation trajectory dataset</i>
                        refers to a collection of trajectories represented as a 3D tensor (i.e., all trajectories are of equal length), where the first axis corresponds to the number
                        of trajectories, the second to the time steps, and the third to the state representation of a dynamic agent.

                            A <i>trajectory</i> represents a dynamic agent's position profile, typically in a two-dimensional plane, over a given period. An agent refers to any observable dynamic object whose position is being
                            tracked, such as humans, mobile robots, human-driven vehicles, autonomous
                            vehicles, or cyclists, whose states can be computed.

                            An agent's <i>states</i> are derived from its tracked trajectory cues, such as position and head orientation,
                            and may include additional time-varying attributes like velocity, acceleration,
                            or actions. Each agent may belong to an <i>observable class</i>, which characterizes
                            the agent type (e.g., pedestrian or car in a road scenario) or the agent's
                            ongoing activities (e.g., transporting an object in an industrial setting).
                            In this case, all trajectories of an agent belong to the same class. Moreover, a
                            trajectory may belong to a learnable class denoted by <i>data-driven class</i>, which
                            relies on unsupervised trajectory cues processing. An agent can also per-
                            form fine-grained <i>actions</i>, which may or may not be unique to its class and
                            form part of its state as they vary over time.
                          </p>

                          <p class="describe-projects">
                            <i>Trajectory prediction</i> or <i>forecasting</i> involves estimating future states, potentially with a different configuration from the observed states, based on past
                            state observations and relevant contextual information, such as the locations
                            of other agents or obstacle maps. The prediction spans a predefined <i>prediction horizon</i>, which is the period from the last observed time step to the final point
                            in time for which predictions are made. We use the terms observed trajectory and tracklet interchangeably to refer to the sequence of observed states.
                            A <i>tracklet</i> spans a predefined <i>observation horizon</i>, which covers the period
                            from the first time step to the last observed time step. <i>Joint trajectory and action prediction</i> involves predicting both future trajectories and the sequence of
                            future actions simultaneously. <i>Class-conditioned trajectory prediction or forecasting</i> involves trajectory prediction conditioned on the corresponding class.
                            <i>Action-conditioned trajectory prediction or forecasting</i> refers to trajectory prediction where the observed states include the observed sequence of fine-grained
                            actions. <i>Multi-task trajectory and action prediction or forecasting</i> refers to the
                            joint prediction of future trajectories and action sequences, where the observed
                            inputs may optionally include the sequence of observed fine-grained actions.
                          </p>

                        <p class="describe-projects">
                          Dynamic agents generate trajectory data, each denoted as \(A_i\) and associated with an observable class \(c_{A_i}\).

Agent trajectories are converted into tracklets of fixed-length \(\mathbf{S} = (\mathbf{s}_t)_{t=1}^{O}\). 

The states \(\mathbf{s}_t\), depending on the dataset, trajectory modeling task, and predictive model, may consist of various configurations: only 2D velocities \(\mathbf{s}_t = (\dot{x}_t, \dot{y}_t)\);
2D positions and velocities, i.e., \(\mathbf{s}_t = (x_t, y_t, \dot{x}_t, \dot{y}_t)\); or including the action \(a_t\), i.e., \(\mathbf{s}_t = (x_t, y_t, \dot{x}_t, \dot{y}_t, a_t)\).

Action labels represent an agent's fine-grained actions at each time step from a predefined set of actions \(\mathcal{A}\).

In contrast to observable classes, which remain constant for all trajectories of an agent, action labels can vary at each time step, influencing human trajectory and capturing its heterogeneity. 

The \emph{future} of an observed tracklet consists of 2D velocities, \(\mathbf{Y_S} = ((\dot{x_t}, \dot{y_t}))^{T_P}_{t=O+1}\) of length \(L = T_P - O\), which are subsequently converted into future positions \(\mathbf{P_S}\). 

The future sequence of actions temporally aligned with \(\mathbf{Y_S}\) is denoted by \(\mathbf{a_S} = (a_t)_{t=O+1}^{T_P}\), \(a_t\in \mathcal{A}\).
                        </p>
                        


           

                          


                        </p>

                      <p class="describe-projects">
                        <h5 style="color:black">3. Contributions and Insights</h5>
                        <p class="describe-projects">
                          The main intuition of this thesis is that robots share space with dynamic agents in anthropocentric environments. The behaviors of dynamic agents are shaped 
                          by a complex interplay between external and internal factors. 
                          External factors include the physical environment, social norms, and interactions with other agents. 
                          Internal factors encompass individual goals, intentions, and psychological states.

                        </p>

                        <p class="describe-projects">
                          This thesis addresses the challenges of discovering and modeling
                          trajectory heterogeneity, a phenomenon arising from these
                          factors, as <b>trajectory classes</b>, which group trajectories based on
                          perceived appearance or trajectory cues.

                          These classes can originate from two primary sources: <i>observable
                          classes</i>, defined by human semantics and accessible via
                          perception systems, and <i>data-driven classes</i>, which are
                          automatically learned from the structure and dynamics of the
                          trajectory data.

                          We find these classes in human motion trajectories datasets, where
                          observable classes can be detected through robot perception and data-
                          driven classes are directly found from the data.

                          External and internal factors affect measurable trajectory
                          cues, such as velocity, acceleration, heading, ongoing action,
                          etc., which can be used to detect/infer the trajectory classes.

                          Finally, we incorporate trajectory classes both observable and data-
                          driven into trajectory prediction methods to enhance trajectory
                          predictions.
                          The figure below depicts the main concepts of this thesis.

                          </p>

                            <img class="img-responsive" src="images/projects/phd/high_level_introduction.png" style="width: 100%" alt="PhD Main Concepts">


                           <p class="describe-projects">
                            The main contributions of this thesis are:
                            
                              <ul>
                                <li> <a href="https://journals.sagepub.com/doi/full/10.1177/02783649241274794" class="tooltip-test">THÖR-MAGNI</a>, a large-scale trajectory dataset capturing diverse human activities in industrial robotics environments, including fine-grained action annotations, <a href="https://ieeexplore.ieee.org/abstract/document/10973897/" class="tooltip-test">THÖR-MAGNI <i>Act</i></a>.</li>
                                <li>Deep learning-based trajectory prediction models conditioned on observable classes, highlighting trade-offs between data efficiency and performance under class imbalance. The respective paper can be found <a href="https://ieeexplore.ieee.org/abstract/document/10545544" class="tooltip-test">here</a>.</li>
                                <li>A novel <a href="https://ieeexplore.ieee.org/abstract/document/10069775" class="tooltip-test">Self-Conditioned GAN</a> to learn data-driven trajectory clusters aligned with generative modeling objectives, enhancing prediction accuracy for underrepresented behaviors.</li>
                                <li>A <a href="https://ieeexplore.ieee.org/abstract/document/10422479" class="tooltip-test">multi-stage prediction framework</a> that explicitly conditions predictions on trajectory clusters, yielding probabilistically informed forecasts.</li>
                              </ul> 
                           </p>

                           <hr class="section-divider"> 
                           <p class="describe-projects">
                              <div id="wrapper">
                              <b>THÖR-MAGNI</b> came to fill in a gap in motion trajectories datasets.
                              
                                The THÖR-MAGNI data collection is designed
                                around systematic variation of environmental factors to allow
                                building cue-conditioned models of human motion and
                                verifying hypotheses on factor impact. To that end, we
                                propose 5 scenarios in which the participants, in addition
                                to primary navigation, need to move objects, interact
                                with each other and the robot, and respond to remote instructions. 
                                The dataset includes differential and omnidirectional robot navigation, 
                                semantic zones, direction signs in the
                                environment, and many other aspects. We provide position
                                and head orientation for each moving agent, as well as 3D
                                lidar scans and gaze tracking. Finally, we provide tools to
                                <a href="https://github.com/tmralmeida/magni-dash" class="tooltip-test">visualize</a> the dataset's multiple modalities and <a href="https://github.com/tmralmeida/thor-magni-tools" class="tooltip-test">preprocess</a> the
                                trajectory data. In total, THÖR-MAGNI
                                captures 3.5 hours of
                                motion of 40 participants over 5 days of recording, which is
                                available for <a href="https://zenodo.org/records/13865754" class="tooltip-test">download</a>.
                                <img class="img-responsive" src="images/projects/phd/thor-magni.gif" style="width: 100%" alt="THÖR-MAGNI data collection.">


                                The data collection implies people performing different <i>roles</i> (i.e., activities tailored to industrial tasks). In addition, 
                                we include a robot in the scene as a static or dynamic obstacle affecting the semantic layout of the environment. Finally, some scenarios 
                                include conditions to study specific aspects or factors affecting human motion. The table below summarizes the data collection.

                                <img class="img-responsive" src="images/projects/phd/thor-magni-scenarios.jpg" style="width: 100%" alt="THÖR-MAGNI scenarios overview.">
                                <b>Learning Outcomes:
                                The observable classes in THÖR-MAGNI underlying the human roles demonstrate distinct motion patterns that could be important for trajectory prediction in robotics environments.
                              </b>

                              
                                </div>
                            </p>
                            <hr class="section-divider"> 

                    
                            <p class="describe-projects">
                              <div id="wrapper">
                            Building on the THÖR-MAGNI data collection, we <b>propose deep learning methods for class-conditioned 
                            trajectory prediction</b>. We also analyze these methods in data settings os particular interest to 
                            robotics and autonomous driving settings: (1) class imbalancing and (2) low data regimes (i.e., non-uniform class distributions).

                            The proposed models follow an encoder-decoder structure where the encoder processes the observed tracklet and the decoder generates the future trajectory.
                            We studied LSTMs and Transformers as encoders and incorporate an embedding layer to process the observable class information.

                            We compare these models to a pattern-based approach based on <a href="https://ieeexplore.ieee.org/abstract/document/10342031" class="tooltip-test">Maps of Dynamics</a>.
                            The figure below depicts the proposed models.
                            <img class="img-responsive" src="images/projects/phd/ral_models.png" style="width: 100%" alt="Deep learning models.">

                            In a nutshell, from the conducted experiments, we have found that specific methods are more suitable for particular data settings. The figure bellow illustrates a model selection decision tree 
                            to guide the choice of the most appropriate model based on the data characteristics.

                            Although trajectory predictors benefit from observable classes globally, they may induce ambiguity in the representation of motion patterns because they are not solely tied to trajectory cues.
                             This is especially evident when different classes share similar motion patterns, resulting in inaccurate representations and reduced prediction accuracy.

                             <img class="img-responsive" src="images/projects/phd/ral_model_selection.png" style="width: 100%" alt="Model selection decision tree.">
                            <b>Learning Outcomes: Maps of Dynamics approaches have an edge over deep generative methods in imbalanced data scenarios and over single-output deep learning methods in low-data regimes.
                              Observable classes may be ambiguous representations of underlying motion patterns, as they are not solely tied to trajectory cues.
                            </b>
                          
                            </div>
                             </p>
                             <hr class="section-divider"> 

                             <p class="describe-projects">
                              <div id="wrapper">
                            To overcome some of the limitations of observable classes, we introduce <b>fine-grained action annotations in THÖR-MAGNI</b>, resulting in the THÖR-MAGNI <i>Act</i> dataset.
                            THÖR-MAGNI <i>Act</i> contains 8.3 hours of fine-grained actions aligned with motion cues.
                            <img class="img-responsive" src="images/projects/phd/thor-magni-act-sample.gif" style="width: 100%" alt="THÖR-MAGNI Act sample.">
                            In the end, the dataset contains 14 unique actions: 
                            $$
                            \begin{aligned}
                            \mathcal{A} = \{ & Walk, DrawCard, ObserveCardDraw, WalkLO, \\ 
                            &  PickBucket,  WalkBucket, DeliverBucket, PickBox, \\
                            & WalkBox, DeliverBox, PickStorageBin, \\
                            & WalkStorageBin, DeliverStorageBin, HRI \} 
                            \end{aligned}
                            $$

                            The intuition is that such fine-grained actions can incorporate the sequence of input states reducing ambiguity as they decompose the observable class in a time-varying sequence of actions. 
                            To that end, such actions must be also diverse and heterogeneous to capture the complexity of human motion. The figure below illustrates the statistics of motions cues, such as acceleration, velocity,
                            and navigation distance across the actions in THÖR-MAGNI <i>Act</i>.
                            <img class="img-responsive" src="images/projects/phd/thor-magni-act-stats.png" style="width: 100%" alt="Motion cues statistics.">


                            To show that actions can be powerful cues for trajectory prediction, we propose two predictive systems based on the single-output methods presented earlier.
                            First, by augmenting the input state representation, we introduce an additional dimension to the state representation, the action. The new state representation is processed by
                            the encoder, and the resulting vector goes directly to the decoder or can be concatenated with the observable class. Then, the trajectory decoder processes the concatenated vector and 
                            outputs the sequence of velocities. We use the mean squared error loss function to train this model.

                            We also studied a multi-task learning model (green) where future sequences of actions are predicted. 
                            To that end, we add a decoder for future sequences of actions trained using binary cross-entropy loss for multi-class classification problems. 





                            <img class="img-responsive" src="images/projects/phd/thor-magni-act-models.png" style="width: 100%" alt="Models.">

                            From the experiments, we have found that both action-conditioned and multi-task models outperform the observable class-conditioned model and the baseline model that does not use any class information.
                            <img class="img-responsive" src="images/projects/phd/thor-magni-act-preds.png" style="width: 100%" alt="Predictions.">


                            

                            <b>Learning Outcomes: Actions can enhance trajectory prediction by mitigating some of the ambiguity present in observable classes.</b>

                          
                            </div>
                             </p>
                             <hr class="section-divider"> 



                              <p class="describe-projects">
                              <div id="wrapper">
                           Actions and observable classes are not always available in real-world scenarios. Moreover, such systems may be erroneous, and these errors can propagate to the downstream predictor. 
                           
                           To overcome these limitations, we propose to learning <b>data-driven trajectory classes</b>, which depend solely on trajectory cues.

                           
                           First, we studied which segment of the trajectory is most suitable for clustering in alignment with the prediction objective.  
                           
                           We found that clusters based solely on the future \(\mathbf{Y_S}\) or the complete trajectory \(\mathbf{S} \oplus \mathbf{Y_S}\) are the most suitable for trajectory prediction.
                           The figure below illustrates predictions conditioned on clusters derived from different trajectory segments.
                          
                           <img class="img-responsive" src="images/projects/phd/conditioned_predictions.png" style="width: 100%" alt="Conditioned predictions.">

                              Next, we propose a novel Self-Conditioned GAN (SC GAN) to learn trajectory clusters aligned with generative modeling objectives.
                              This framework based on Generative Adversarial Networks (GANs) assumes that the representations learned in the discriminator's 
                              feature space are more suitable for clustering than the raw trajectory data.
                              The figure below illustrates the SC-GAN architecture. It allows for the configuration of entire or just future trajectory segments.

                              <img class="img-responsive" src="images/projects/phd/scgan.png" style="width: 100%" alt="Self-conditioned GAN.">
                          
                                Another advantage from Self-Conditioned GAN is that it can generate diverse trajectory samples by conditioning the generator on different clusters.
                                Traditional GANs generate diverse samples by varying the input noise vector relying on the input data distribution. However, when the data distribution is rather skewed or imbalanced, GANs may suffer from mode collapse, generating samples that are too similar to each other.
                                
                                On the other hand, in SC-GAN, the generator is conditioned on cluster classes, which represent distinct motion patterns.
                                This conditioning allows the generator to produce samples that are not only diverse but also aligned with specific trajectory patterns represented by the clusters.
                                The figure below illustrates a traditional GAN result (left) versus diverse trajectory samples generated by SC GAN (right) when conditioned on different clusters.
                                <img class="img-responsive" src="images/projects/phd/mode_collapse.png" style="width: 100%" alt="Mode collapse in GANs.">
                        

                            <b>Learning Outcomes: Full- and future-driven clusters are the most suitable for trajectory prediction. 
                              Self-conditioned GAN is a powerful clustering method connecting the clustering and the trajectory prediction objectives.</b>

                          
                            </div>
                             </p>
                             <hr class="section-divider"> 

                             <p class="describe-projects">
                              <div id="wrapper">
                          Based on these insights, we propose using the clusters learned by SC-GAN to implicitly improve the performance of trajectory predictors in heterogeneous data settings.
                          This would mitigate the mode-collapse problem in GAN-based forecasters - (1) in the figure below.
                          
                          Additionally, we propose a multi-stage prediction framework that explicitly conditions trajectory predictions on data-driven clusters - (2) in the figure below.
                          
                          <img class="img-responsive" src="images/projects/phd/data_driven_prediction.png" style="width: 100%" alt="Data-driven classes for trajectory prediction.">

                          For the approach (1), we simply use the clustering space learned by SC-GAN and the results obtained by the SC GAN to force other 
                          GAN-based forecaster to learn specific motion patterns. 
                          
                          To that end, we propose a new loss function that forces the generator to predict best the trajectories coming from clusters where the SC GAN failed the most.
                          This is done by computing the mean squared error (MSE) between the predicted trajectory and the ground truth trajectory for each cluster and penalizing it more 
                          based on the average displacement error and the final displacement error. The former measures the average L2 distance between the predicted trajectory and the ground truth trajectory over all time steps.
                          The latter measures the L2 distance between the final predicted position and the corresponding ground truth.
                          These two metrics are widely used in trajectory prediction to evaluate the accuracy of predicted trajectories.
                          The figure below illustrates predictions from penalized prediction systems versus the baseline model (without penalization).

                           <img class="img-responsive" src="images/projects/phd/implicit_preds.png" style="width: 100%" alt="Predictions from penalized prediction systems.">

                          For the approach (2), we propose a multi-stage prediction framework that explicitly conditions trajectory predictions on data-driven clusters.
                          The first step is to transform the data into displacements, for example, to make it more generalizable to other datasets.
                          Then, we cluster these displacements using the full-cluster SC GAN, k-means, or time-series k-means.
                          Next, we train a deep generative model conditioned on the correct cluster classes to produce the future sequence of states. During inference, we sample one trajectory per cluster.
                          With one prediction per cluster class, we rank the future trajectories using distance metrics and transform them into a probabilistic space.
                          How do we do that? We apply a soft-argmax function based on the distance: (1) between the produced sample and the corresponding centroid; or, between the produced sample and the \(N_{neig}\) neighbors from the corresponding cluster.
                          This approach follows the intuitive idea that a sample produced based on the correct cluster will always be closer to the cluster than the samples produced from other clusters and the samples within those clusters, as can be seen in the figure below.
                          Our prediction framework proved to be more effective than unconditional baselines. Also, our predictions raking methods are both efficient and effective comparatively to neural networks.


                           <img class="img-responsive" src="images/projects/phd/multi_stage_framework.png" style="width: 100%" alt="Multi-stage prediction framework.">

                            <b>Learning Outcomes: SC GAN can help mitigating the mode collapse problem in GAN-based trajectory forecasters.
                              The multi-stage prediction framework effectively leverages data-driven clusters to enhance trajectory prediction accuracy and our predictions ranking methods are both efficient and effective.
                            </b>

                          
                            </div>
                             </p>




                          


                        </p>
                    </div>
                    <div class="modal-footer">
                      <button id="prev_shower" type="button" class="btn btn-primary" onclick="toggle_modals('#modal-rl', '#modal-shower')">Previous</button>
                      <button type="button" class="btn btn-primary" disabled>Next</button>
                      <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                    </div>
                  </div>
                </div>
              </div>

            </div>
          </div>





          <!-- Reinforcement Learning -->
          <div class="card">
            
            <h2 class="card-header">Reinforcement Learning</h2>
            <div class="card-body">
                <p class="card-text"> Similarly to what I have done with the bag of models for Deep Learning, here I present my Reinforcement Learning studies.
                  It comprises some playground projects where I use reinforcement learning to train agents... </p>
              <button id="read_more_rl" type="button" class="btn btn-primary" data-toggle="modal" data-target="#modal-rl">
                  Continue reading
              </button>
              <div class="modal fade" id="modal-rl" tabindex="-1" role="dialog" aria-labelledby="bag-reinforcement-title" aria-hidden="true">
                <div class="modal-dialog modal-lg" role="document">
                  <div class="modal-content">
                    <div class="modal-header">
                      <h4 class="modal-title" id="shower-modal-title">Reinforcement Learning</h4>
                      <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                        <span aria-hidden="true">&times;</span>
                      </button>
                    </div>
                    <div class="modal-body">
                      
                      <p class="describe-projects">
                        <h5 style="color:black">1. Context</h5>
                        <p class="describe-projects">
                        Similarly to what I have done with the studies for Deep Learning, here I present my Reinforcement Learning studies.
                        It comprises some playground projects where I use reinforcement learning to train agents to accomplish some behavior required from a given task withing a given environment.
                        The whole idea is that we make an agent interact with an environment and learn from it.
                        That environment can be a game, a robot, or even a real-world problem.
                        The idea is that such environment is able to return a reward to the agent, which is the feedback that the agent receives from its actions.
                        The agent's goal is to maximize the reward it receives from the environment.
                        The agent learns by trial and error, exploring the environment and trying different actions to see what works best.
                        To that end, the agent's goal is to learn a policy that maps states to actions, so it can take the best action in each state.
                        To make the problem easier to solve, neural networks can be used to approximate the policy.
                        In that case, Deep Reinforcement Learning emerges as the research topics to study. 
                      </p class="describe-projects">
                      <p class="describe-projects">
                        <h5 style="color:black">2. Outline</h5>
                        <p class="describe-projects">
                        Everything started by the study of the Sutton and Barton's book that one can find <a href="https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf" class="tooltip-test">here</a>.
                        Then, I attended the free recorded lectures of Hado van Hasselt, who follows the same book but provides deeper and insightful explanations of some of the examples of the book.
                        I defend the principle of learning by doing. In this way, I have watched the full Reinforcement Learning course from <a href=https://deeplizard.com/learn/playlist/PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv class="tooltip-test">deeplizard</a>. 
                        Finally, I followed the <a href="https://spinningup.openai.com/en/latest/index.html" class="tooltip-test">Spinning Up documentation</a>, which I strongly suggest for those 
                        who aim to learn Deep Reinforcement Learning. In doing so, I have been deploying a few of the most well-known Deep Reinforcement Learning algorithms and doing some funny experiments. 
                        </p>
                      </p>
                      <p class="describe-projects">
                        <h5 style="color:black">3. Practice</h5>
                        <p class="describe-projects">
                          This Deep RL library of algorithms is an ongoing project. So, at this time there is the CartPole environment solved by the following PyTorch implementations:
                          <div id="wrapper">
                            <ul>
                              <li>Vanilla Policy Gradient or REINFORCE</li>
                              <li>DQN</li>
                              <li>A3C</li>
                            </ul> 
                          </div>
                          </p>
                          <p class="describe-projects">

                            During my experiments I have implemented both default states and images observations. I could not make it for the A3C due to the limitation of the render provided 
                            by OpenAI. In the meanwhile, for those interested, I suggest to check the official repo for more information and (personal) explanations.
                            <b>You can check the repository at </b><a href="https://github.com/tmralmeida/road2-rl/tree/main/deep-rl" class="tooltip-test">tmralmeida</a>. 
                            
                            I also had the opportunity of using one of these RL frameworks in one of the courses I took during my Ph.D. Therefore, I was asked to develop a vacuum cleaner 
                            AI-based agent whose goal is to clean a grid world environment. In this environment, the AI-based agent may encounter obstacles and dirty cells. 
                            In this way, I've trained REINFORCE to solve this problem, and the results can be found on this <a href="https://github.com/tmralmeida/wasp-AI-ML-m1" class="tooltip-test">repo</a>.

                          </p>


                        </p>
                    </div>
                    <div class="modal-footer">
                      <button id="prev_shower" type="button" class="btn btn-primary" onclick="toggle_modals('#modal-rl', '#modal-shower')">Previous</button>
                      <button type="button" class="btn btn-primary" onclick="toggle_modals('#modal-rl', '#modal-phd')">Next</button>
                      <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                    </div>
                  </div>
                </div>
              </div>

            </div>
          </div>



          <!-- Smart shower -->
          <div class="card">
          <h2 class="card-header">Smart Shower App</h2>
            <div class="card-body">
                <p class="card-text">
                  <p class="card-text"> This is a homemade project that emulates a smart hands-free shower/tap. This POC is based on a low-cost prototype 
                  composed of a Raspberry Pi 3b+, a picamera, an ultrasonic, 3 LEDs, and a servo motor. </p>
              <button id="read_more_shower" type="button" class="btn btn-primary" data-toggle="modal" data-target="#modal-shower">
                  Continue reading
              </button>
              <div class="modal fade" id="modal-shower" tabindex="-1" role="dialog" aria-labelledby="bag-shower-title" aria-hidden="true">
                <div class="modal-dialog modal-lg" role="document">
                  <div class="modal-content">
                    <div class="modal-header">
                      <h4 class="modal-title" id="shower-modal-title">smart-shower Project</h4>
                      <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                        <span aria-hidden="true">&times;</span>
                      </button>
                    </div>
                    <div class="modal-body">
                      
                      <p class="describe-projects">
                        <h5 style="color:black">1. Description</h5>
                        <p class="describe-projects">
                        This is a homemade project that emulates a smart hands-free shower/tap. This is a POC based on a low-cost prototype composed of a Raspberry Pi 3b+, a picamera, an ultrasonic, 3 LEDs, 
                        and a servo motor. At the time of publication of this idea, there are difficult times in the world due to Coronavirus. This virus is characterized by being difficult to control due 
                        to its easy spread. Therefore, the idea behind this smart system is reducing the spread of diseases such as Coronavirus in public bathing facilities through totally hands-free and 
                        intelligent showers and taps.
                      </p class="describe-projects">
                      <p class="describe-projects">
                        <h5 style="color:black">2. Idea</h5>
                        <p class="describe-projects">
                        There are a lot of hands-free taps and showers but I have never seen one that could control the flow and temperature of water smartly and intuitively. Hence, the objective here is 
                        to control both flow and temperature by the location of the hands in relation to the tap. It is as if we placed an XY-plane coordinate system in front of the tap sensor, with the x-axis 
                        (horizontal) being the temperature and the y-axis (vertical) the flow. Then, the more to the right our hand is on the tap, the hotter the temperature will come out and, similarly for 
                        the vertical axis, the higher above the tap, the higher the flow.
                        </p>
                      </p>
                      <p class="describe-projects">
                        <h5 style="color:black">3. Practice</h5>
                        <p class="describe-projects">
                          <img class="img-responsive" src="images/projects/smart-shower/fluxo.png" style="width: 70%" alt="Application workflow">
                        </p>
                        <p class="describe-projects">
                          According to the image above that represents the entire workflow, the raspberry and arduino are always in communication. Therefore, arduino continuously sends the distance to 
                          the raspberry until it is less than 15. 
                          At this point, one hand is in front of the sensor, which triggers the camera. Then the raspberry computes the code that corresponds to the hand location. This code is sent to the 
                          arduino that yields the respective outputs to each electronic device (LEDs and motor). Please, note that:
                          <div id="wrapper">
                          <ul>
                            <li>Motor 80 corresponds to a closed tap;</li>
                            <li>Motor 10 corresponds to a fully open tap;</li>
                            <li>Motor 45 corresponds to a partly open tap;</li>
                            <li>Yellow LED corresponds to mild water;</li>
                            <li>Blue LED corresponds to cold water;</li>
                            <li>Red LED corresponds to hot water.</li>
                          </ul> 
                        </div>
                        <p class="describe-projects">
                          Thus, the final system would be composed by a 3D ultrasonic sensor that in this prototype is represented by the usage of a simple ultrasonic and a camera.
                        </p>
                        <p class="describe-projects">
                          <b>You can check the repository at </b><a href="https://github.com/tmralmeida/smart-shower" class="tooltip-test">tmralmeida</a>. Below there is a representative video.
                        </p>
                        <iframe width="100%" height="315" src="https://www.youtube.com/embed/jOX66OmSYe4" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                        </p>
                        

                      </p>
                    </div>
                    <div class="modal-footer">
                      <button id="prev_shower" type="button" class="btn btn-primary" onclick="toggle_modals('#modal-shower', '#modal-bag')">Previous</button>
                      <button type="button" class="btn btn-primary" onclick="toggle_modals('#modal-shower', '#modal-rl')">Next</button>
                      <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                    </div>
                  </div>
                </div>
              </div>

            </div>
          </div>




            <div class="card">
              <h2 class="card-header">Deep Learning</h2>
              <div class="card-body">
                  <h5 class="card-title">Mar - July 2020</h5>
                  <p class="card-text">This is a guide for Deep Learning practitioners. It covers Tensorflow and Pytorch
                  techniques to train the best-known models for Image Classification and...</p>
                <button id="read_more_bag" type="button" class="btn btn-primary" data-toggle="modal" data-target="#modal-bag">
                    Continue reading
                </button>
                <div class="modal fade" id="modal-bag" tabindex="-1" role="dialog" aria-labelledby="bag-modal-title" aria-hidden="true">
                  <div class="modal-dialog modal-lg" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <h4 class="modal-title" id="bag-modal-title">Deep Learning Studies</h4>
                        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                          <span aria-hidden="true">&times;</span>
                        </button>
                      </div>
                      <div class="modal-body">
                        
                        <p class="describe-projects">
                          This is a guide for Deep Learning practitioners. It covers Tensorflow and Pytorch techniques to train the best-known models for Image Classification and Object Detection fields. At the beginning
                          of my journey of learning this topic in practice, the most difficult thing for me was filtering out all the information, because every practitioner has one repository
                          and it seems that they have results but their code is too complex for a beginner. Therefore, I started with a <a href="https://www.coursera.org/specializations/tensorflow-in-practice"
                          class="tooltip-test">Tensorflow Specialization</a> and as I was learning, I was doing my test cases for myself. In my opinion, the easiest way to start is with Image Classification because 
                          it does not resort as much as effort as the other fields. The effort here is important, because it is an effort related to the complexity of conceiving the model in practice, so less
                          effort means a more understandable and easier code. Thus, I started to download one dataset (CINIC10), then I tried to replicate the models training, which I was studying 
                          through the respective papers (I went from AlexNet to MobileNet). The code is not the most efficient one but it was done by a beginner so I hope that it is clear enough.
                        </p class="describe-projects">
                        <p class="describe-projects">
                          After Image Classification, I wanted to study Object Detection, which seems a trendy Computer Vision task but it was difficult to assimilate all the little tricks behind each 
                          choice of the authors of the most well-known architectures. At the same time, in my work the opportunity of working also in Object Detection arose. So, it was a win-win situation.
                          First, I attend the <a href="https://deeplizard.com/learn/video/v5cngxo4mIg" class="tooltip-test">deeplizard</a> course about Pytorch because I wanted to know all the decent possibilities
                          I had in terms of Deep Learning frameworks. Hence, Pytorch was used to study those architectures (from Faster R-CNN up to YOLOv4).
                        </p>
                        <p class="describe-projects">
                          Now, you can decide which of the branches of this project you want to check:
                       
                          <div class="btn-group" role="group">
                            <button id="btnGroupDrop1" type="button" class="btn btn-secondary dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                              Fields
                            </button>
                            <div class="dropdown-menu" aria-labelledby="btnGroupDrop1">
                              <a class="dropdown-item" onclick="toggle_modals('#modal-bag', '#modal-classification')">Image Classification</a>
                              <a class="dropdown-item" onclick="toggle_modals('#modal-bag', '#modal-detection')">Object Detection</a>
                            </div>
                          </div>
                      </p>
                      </div>
                      <div class="modal-footer">
                        <button id="prev_bag" type="button" class="btn btn-primary" onclick="toggle_modals('#modal-bag', '#modal-faster')">Previous</button>
                        <button id="fol_bag" type="button" class="btn btn-primary" onclick="toggle_modals('#modal-bag', '#modal-shower')">Next</button>
                        <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                      </div>
                    </div>
                  </div>
                </div>

              </div>
            </div>

            <!-- Bag of models fields -->
            <!-- Classification -->
            <div class="modal fade" id="modal-classification" tabindex="-1" role="dialog" aria-labelledby="classification-modal-title" aria-hidden="true">
              <div class="modal-dialog modal-lg" role="document">
                <div class="modal-content">
                  <div class="modal-header">
                    <h4 class="modal-title" id="classification-modal-title">Image Classification</h4>
                    <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                      <span aria-hidden="true">&times;</span>
                    </button>
                  </div>
                  <div class="modal-body">
                    <p class="describe-projects">
                      First of all, for data loading I used in every model the <a href="https://www.tensorflow.org/guide/data" class="tooltip-test">tf.data</a> module. It allows to create
                      a full pipeline that aggregates: loading from disk, data augmentation, and batch formation. I did not go too deep into augmentation because the objective at this point
                      would be to practice the models creation and try to understand the various ways of doing it by using Tensorflow2.0. In my opinion, there are three global ways to deploy a 
                      Tensorflow model, whose usage depends on the architecture's layout. If the model is straightforward (the easiest ones) we can use the 
                      <a href="https://www.tensorflow.org/api_docs/python/tf/keras/Sequential" class="tooltip-test">Sequential API</a>; 
                      on the other hand, if the model resorts layers concatenation and "parallel operations" (more complex models), we should use the 
                      <a href="https://www.tensorflow.org/guide/keras/functional" class="tooltip-test"> Functional API</a>; finally, if we want a fully-customizable foward 
                      propagation we can use <a href="https://www.tensorflow.org/guide/keras/custom_layers_and_models" class="tooltip-test">Model subclassing</a>. 
                    </p>
                    <p class="describe-projects">
                      During this study, I just used the Sequential API for the easiest models and the Functional API for the more complex ones. Therefore, the first three models - AlexNet, ZFNet 
                      and VGG16 - were created under the Sequential API due to their simple design. The remaining models - ResNet18, GoogLeNet, Xception and MobileNet - were designed through the Functional
                      API.
                    </p>
                    <p class="describe-projects">
                      Theoretically, it is important to highlight some key points in the history of Convolutional Neural Networks for Image Classification, which are now used or have an influence on
                      the most modern architectures:

                      <ul class="list-group list-group-flush" style="color:black;font-weight:300;">
                        <li class="list-group-item"><b>AlexNet</b> is the first Convolutional Neural Network that obtained a quite important result in the ImageNet challenge;</li>
                        <li class="list-group-item"><b>ZFNet</b> showed how it would be possible to improve the network's layout by visualizing what is going on inside of it;</li>
                        <li class="list-group-item"><b>VGG16</b> showed that deeper convolutional neural networks can be more accurate than shallower networks; </li>
                        <li class="list-group-item">The more layers a neuronal network has, the harder it is to train. Thus, <b>ResNet</b> showed how it is possible to train deep neural networks 
                          in a simpler fashion by applying residual blocks with skip connections. The image below demonstrates this design choice (from <a href="https://d2l.ai/chapter_convolutional-modern/resnet.html"
                          class="tooltip-test">Dive Into Deep Learning</a>); the left image represents the original residual block and the right image illustrates the residual block with 
                          the respective skip connection - trick that makes the network training easier.</li>
                        </ul> 
                      <img class="img-responsive" src="images/projects/bag-of-models/classification/resblock.png" width="60%" alt="resblock and skip connections">
                    </p>
                    
                    <p class="describe-projects">
                      <b>You can check all notebooks at </b><a href="https://github.com/tmralmeida/bag-of-models/tree/master/CNNs/1-Image_Classification" class="tooltip-test">tmralmeida</a>.
                    </p>
                    
                   
                  </div>
                  <div class="modal-footer">
                    <button type="button" class="btn btn-primary" onclick="toggle_modals('#modal-classification', '#modal-bag')">Previous</button>
                    <button type="button" class="btn btn-primary" onclick="toggle_modals('#modal-classification', '#modal-detection')">Next</button>
                    <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                  </div>
                </div>
              </div>
            </div>

            <!-- Detection -->
            <div class="modal fade" id="modal-detection" tabindex="-1" role="dialog" aria-labelledby="detection-modal-title" aria-hidden="true">
              <div class="modal-dialog modal-lg" role="document">
                <div class="modal-content">
                  <div class="modal-header">
                    <h4 class="modal-title" id="detection-modal-title">Object Detection</h4>
                    <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                      <span aria-hidden="true">&times;</span>
                    </button>
                  </div>
                  <div class="modal-body">
                    <p class="describe-projects">
                      As mentioned before, the study of object detection in this project came at the same time as a task that I had to complete professionally. At that time, I was studying object detection architectures in 
                      different road environments for autonomous driving. Therefore, after studying several available datasets, the one that proved to be most representative in terms of the quality/diversity of information ratio was 
                      the <a href="https://bair.berkeley.edu/blog/2018/05/30/bdd/" class="tooltip-test">BDD100K</a>. This dataset is composed of several different types of annotations, and the one we worked on 
                      (road object bounding boxes) is divided into 10 different classes: bus, light, sign, person, bike, truck, motor, car, train, and rider. 
                    </p>
                    <p class="describe-projects">
                      After choosing the dataset, I also study the state-of-the-art of Object Detection arquitectures. From here, two major types of architectures for the Object Detection task arose: proposals networks 
                      and single shot methods. The former is represented by Faster R-CNN, which I had already used in another project. The latter is composed of SSD (Single Shot Detection) and all versions of YOLO. Hence,
                      these are the architectures that I would use to perform this study. Since I had deadlines to present results in my work, the models that I used here are not originally made by me, but based on works of
                      other authors as I will reference later.
                    </p>

                    <!-- Faster -->
                    <p class="describe-projects">
                      <h5 style="color:black">1. Faster R-CNN</h5>
                          <p class="describe-projects">
                            Faster RCNN is one of the most widely used deep learning models for object detection. Although, its high-latency comparing to single-shot methods, Faster RCNN is performant 
                            detecting both small and large objects. The authors of this DL architecture divide the overall architecture into 2 modules, however, it is fairer to divide it into 3 modules: 
                            feature maps extractor, RPN (Region Proposals Network) and Fast R-CNN detector.
                            The former is composed of a traditional classification architecture, which is responsible for producing feature maps. In our approach we choose a MobileNetV2 to perform this 
                            task due to its low-latency. After that, a small network slides over the feature maps predicting multiple possible proposals for each of its cells. This small network returns a 
                            lower-dimensional feature, which is then fed to two 1 * 1 convolutional layers. These layers yield the probability of a proposal bounding a target, and the encoded coordinates of
                             each proposal, respectively. Finally, the features that correspond to objects pass through an ROI pooling layer that crops and rescales each feature. During inference, the non-maximum 
                             suppression (NMS) algorithm is computed to filter out the best-located bounding boxes.
                          </p>
                          <p class="describe-projects">
                            The work that we developed here in terms of training and model creation was based on the <a href="https://github.com/pytorch/vision/tree/master/torchvision" class="tooltip-test">torchvision 
                            module</a> of Pytorch framework.
                          </p>
                          <p class="describe-projects">
                            The numeric results for the validation set, based on <a href="http://cocodataset.org/#detection-eval" class="tooltip-test">COCO metrics</a> are represented in the table below. 
                          </p>
                      
                          <table class="tg">
                            <thead>
                              <tr>
                                <th class="tg-0pky"></th>
                                <th class="tg-c3ow">IoU Thresholds</th>
                                <th class="tg-c3ow">Scales</th>
                                <th class="tg-c3ow">maxDets</th>
                                <th class="tg-c3ow">AP/AR values</th>
                              </tr>
                            </thead>
                            <tbody>
                              <tr>
                                <td class="tg-9wq8" rowspan="6">AP</td>
                                <td class="tg-c3ow">[0.50 : 0.05 : 0.95]</td>
                                <td class="tg-c3ow">all</td>
                                <td class="tg-c3ow">100</td>
                                <td class="tg-c3ow">0.202</td>
                              </tr>
                              <tr>
                                <td class="tg-c3ow">0.50</td>
                                <td class="tg-c3ow">all</td>
                                <td class="tg-c3ow">100</td>
                                <td class="tg-c3ow">0.409</td>
                              </tr>
                              <tr>
                                <td class="tg-c3ow">0.75</td>
                                <td class="tg-c3ow">all</td>
                                <td class="tg-c3ow">100</td>
                                <td class="tg-c3ow">0.175</td>
                              </tr>
                              <tr>
                                <td class="tg-c3ow">0.95</td>
                                <td class="tg-c3ow">small</td>
                                <td class="tg-c3ow">100</td>
                                <td class="tg-c3ow">0.050<br></td>
                              </tr>
                              <tr>
                                <td class="tg-c3ow">[0.50 : 0.05 : 0.95]</td>
                                <td class="tg-c3ow">medium</td>
                                <td class="tg-c3ow">100</td>
                                <td class="tg-c3ow">0.243<br></td>
                              </tr>
                              <tr>
                                <td class="tg-c3ow">[0.50 : 0.05 : 0.95]<br></td>
                                <td class="tg-c3ow">large</td>
                                <td class="tg-c3ow">100</td>
                                <td class="tg-c3ow">0.432<br></td>
                              </tr>
                              <tr>
                                <td class="tg-lboi" rowspan="6">AR</td>
                                <td class="tg-c3ow">[0.50 : 0.05 : 0.95]</td>
                                <td class="tg-c3ow">all</td>
                                <td class="tg-c3ow">1</td>
                                <td class="tg-c3ow">0.158</td>
                              </tr>
                              <tr>
                                <td class="tg-c3ow">[0.50 : 0.05 : 0.95]</td>
                                <td class="tg-c3ow">all</td>
                                <td class="tg-c3ow">10</td>
                                <td class="tg-c3ow">0.277</td>
                              </tr>
                              <tr>
                                <td class="tg-c3ow">[0.50 : 0.05 : 0.95]</td>
                                <td class="tg-c3ow">all</td>
                                <td class="tg-c3ow">100</td>
                                <td class="tg-c3ow">0.290</td>
                              </tr>
                              <tr>
                                <td class="tg-c3ow">[0.50 : 0.05 : 0.95]</td>
                                <td class="tg-c3ow">small</td>
                                <td class="tg-c3ow">100</td>
                                <td class="tg-c3ow">0.116</td>
                              </tr>
                              <tr>
                                <td class="tg-c3ow">[0.50 : 0.05 : 0.95]</td>
                                <td class="tg-c3ow">medium</td>
                                <td class="tg-c3ow">100</td>
                                <td class="tg-c3ow">0.355<br></td>
                              </tr>
                              <tr>
                                <td class="tg-c3ow">[0.50 : 0.05 : 0.95]</td>
                                <td class="tg-c3ow">large</td>
                                <td class="tg-c3ow">100</td>
                                <td class="tg-c3ow">0.519<br></td>
                              </tr>
                            </tbody>
                          </table>
                        
                          <p class="describe-projects">
                              Finally, I release videos that demonstrate part of the qualitative results of the trained model in frames acquired in Aveiro roads. One example of those 
                              videos is shown below.
                          </p>                            
                            <iframe width="100%" height="315" src="https://www.youtube.com/embed/o7KSFDoEfW0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                    </p>

                    <!-- SSD -->
                    <p class="describe-projects">
                      <h5 style="color:black">2. SSD512</h5>
                          <p class="describe-projects">
                            Single shot models can process the input faster due to the respective tasks - localization and classification - be done in a single forward fashion. 
                            Here, SSD is presented as well as its results in the validation set of the dataset used in this work. This architecture is characterized by its base 
                            network (or backbone), the usage of multi-scaled feature maps for the detection task, and the respective convolutional predictors. MobileNetV2 was used 
                            to perform the perception of the image features and then was truncated before the classification layers. Hence, some of the final layers of MobileNet 
                            and additional feature layers allow multiple scales predictions. Each of these extra layers can produce a fixed set of detection predictions 
                            using a set of convolutional filters. Finally, the output of the model is the score for a category and the location of the box that bounds the target object.
                          </p>
                          <p class="describe-projects">
                            This work, in terms of code, is based on the one of <a href="https://github.com/qfgaohao/pytorch-ssd" class="tooltip-test">qfgaohao</a>. However, here I did some adaptations
                            to increase the performance of the model. One of them is the 512*512 input size.
                          </p>
                          <p class="describe-projects">
                            Finally, the numeric results for the BDD100K validation set are represented in the table below.
                          </p>
                      
                          <table class="tg">
                            <thead>
                              <tr>
                                <th class="tg-0pky"></th>
                                <th class="tg-c3ow">IoU Thresholds</th>
                                <th class="tg-c3ow">Scales</th>
                                <th class="tg-c3ow">maxDets</th>
                                <th class="tg-c3ow">AP/AR values</th>
                              </tr>
                            </thead>
                            <tbody>
                              <tr>
                                <td class="tg-9wq8" rowspan="6">AP</td>
                                <td class="tg-c3ow">[0.50 : 0.05 : 0.95]</td>
                                <td class="tg-c3ow">all</td>
                                <td class="tg-c3ow">100</td>
                                <td class="tg-c3ow">0.083</td>
                              </tr>
                              <tr>
                                <td class="tg-c3ow">0.50</td>
                                <td class="tg-c3ow">all</td>
                                <td class="tg-c3ow">100</td>
                                <td class="tg-c3ow">0.131</td>
                              </tr>
                              <tr>
                                <td class="tg-c3ow">0.75</td>
                                <td class="tg-c3ow">all</td>
                                <td class="tg-c3ow">100</td>
                                <td class="tg-c3ow">0.085</td>
                              </tr>
                              <tr>
                                <td class="tg-c3ow">0.95</td>
                                <td class="tg-c3ow">small</td>
                                <td class="tg-c3ow">100</td>
                                <td class="tg-c3ow">0.002<br></td>
                              </tr>
                              <tr>
                                <td class="tg-c3ow">[0.50 : 0.05 : 0.95]</td>
                                <td class="tg-c3ow">medium</td>
                                <td class="tg-c3ow">100</td>
                                <td class="tg-c3ow">0.044<br></td>
                              </tr>
                              <tr>
                                <td class="tg-c3ow">[0.50 : 0.05 : 0.95]<br></td>
                                <td class="tg-c3ow">large</td>
                                <td class="tg-c3ow">100</td>
                                <td class="tg-c3ow">0.293<br></td>
                              </tr>
                              <tr>
                                <td class="tg-lboi" rowspan="6">AR</td>
                                <td class="tg-c3ow">[0.50 : 0.05 : 0.95]</td>
                                <td class="tg-c3ow">all</td>
                                <td class="tg-c3ow">1</td>
                                <td class="tg-c3ow">0.068</td>
                              </tr>
                              <tr>
                                <td class="tg-c3ow">[0.50 : 0.05 : 0.95]</td>
                                <td class="tg-c3ow">all</td>
                                <td class="tg-c3ow">10</td>
                                <td class="tg-c3ow">0.093</td>
                              </tr>
                              <tr>
                                <td class="tg-c3ow">[0.50 : 0.05 : 0.95]</td>
                                <td class="tg-c3ow">all</td>
                                <td class="tg-c3ow">100</td>
                                <td class="tg-c3ow">0.093</td>
                              </tr>
                              <tr>
                                <td class="tg-c3ow">[0.50 : 0.05 : 0.95]</td>
                                <td class="tg-c3ow">small</td>
                                <td class="tg-c3ow">100</td>
                                <td class="tg-c3ow">0.005</td>
                              </tr>
                              <tr>
                                <td class="tg-c3ow">[0.50 : 0.05 : 0.95]</td>
                                <td class="tg-c3ow">medium</td>
                                <td class="tg-c3ow">100</td>
                                <td class="tg-c3ow">0.052<br></td>
                              </tr>
                              <tr>
                                <td class="tg-c3ow">[0.50 : 0.05 : 0.95]</td>
                                <td class="tg-c3ow">large</td>
                                <td class="tg-c3ow">100</td>
                                <td class="tg-c3ow">0.334<br></td>
                              </tr>
                            </tbody>
                          </table>
                        
                          <p class="describe-projects">
                            Although a huge difference between the numerical results for the validation set between the two architectures presented so far, this model is also 
                            performant on Aveiro roads. Please, check the video below.
                          </p>    

                          <iframe width="100%" height="315" src="https://www.youtube.com/embed/DGnp-dfRaXU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                   
                    </p>

                    <!-- YOLOV4 -->
                    <p class="describe-projects">
                      <h5 style="color:black">3. YOLOV4</h5>
                          <p class="describe-projects">
                            All YOLO architectures are also single-shot methods, and that is why they achieve high-speed predictions. The authors have been presenting several evolutions, 
                            which is proved by the amount of YOLO versions that exist - 4 until the writing date of this post (YOLO, YOLOv2, YOLOv3, and YOLOv4). This architecture 
                            has always shown low-latency and, therefore, what has been the focus along the various versions is the localization performance.

                            YOLOv4 is composed of a Cross Stage Partial (CSP) Darknet53 with an SPP module, a path-aggregation net (PANet), and a 
                            YOLOv3 head. CSP networks have similar basis and purposes to a DenseNet. Therefore, this type of architectures enhances the features reuse by reducing the 
                            amount of repeated gradient information observed in a DenseNet. To do so, it divides the base feature map, then a part of the channels  passes through a 
                            partial dense block and the other part undergoes to the final partial transition layer. After activation maps production, the only difference between YOLOv3 
                            and YOLOv4 in terms of architecture's layout is the global features concatenation. Instead of the FPN technique, a custom PANet approach is performed.
                            PANet is simply an enhanced version of FPN; after the FPN's block composed of a top-down pathway with lateral connections, PANet also propagates low-level 
                            features through a bottom-up path augmentation block. This block allows the addition (concatenation for YOLOv4) of the FPN resulting features with the output 
                            of those feature maps with 3*3 convolutions, which yields an even better understanding of the low-level features.
                          </p>
                          <p class="describe-projects">
                            This work, in terms of code, is based on the one of <a href="https://github.com/ultralytics/yolov3" class="tooltip-test">Ultralytics</a> with some changes to allow the usage
                            of the Ignite framework.
                          </p>
                          <p class="describe-projects">
                            Finally, the numeric results for the BDD100K validation set are represented in the table below.
                          </p>
                      
                          <table class="tg">
                            <thead>
                              <tr>
                                <th class="tg-0pky"></th>
                                <th class="tg-c3ow">IoU Thresholds</th>
                                <th class="tg-c3ow">Scales</th>
                                <th class="tg-c3ow">maxDets</th>
                                <th class="tg-c3ow">AP/AR values</th>
                              </tr>
                            </thead>
                            <tbody>
                              <tr>
                                <td class="tg-9wq8" rowspan="6">AP</td>
                                <td class="tg-c3ow">[0.50 : 0.05 : 0.95]</td>
                                <td class="tg-c3ow">all</td>
                                <td class="tg-c3ow">100</td>
                                <td class="tg-c3ow">0.105</td>
                              </tr>
                              <tr>
                                <td class="tg-c3ow">0.50</td>
                                <td class="tg-c3ow">all</td>
                                <td class="tg-c3ow">100</td>
                                <td class="tg-c3ow">0.209</td>
                              </tr>
                              <tr>
                                <td class="tg-c3ow">0.75</td>
                                <td class="tg-c3ow">all</td>
                                <td class="tg-c3ow">100</td>
                                <td class="tg-c3ow">0.092</td>
                              </tr>
                              <tr>
                                <td class="tg-c3ow">0.95</td>
                                <td class="tg-c3ow">small</td>
                                <td class="tg-c3ow">100</td>
                                <td class="tg-c3ow">0.053<br></td>
                              </tr>
                              <tr>
                                <td class="tg-c3ow">[0.50 : 0.05 : 0.95]</td>
                                <td class="tg-c3ow">medium</td>
                                <td class="tg-c3ow">100</td>
                                <td class="tg-c3ow">0.223<br></td>
                              </tr>
                              <tr>
                                <td class="tg-c3ow">[0.50 : 0.05 : 0.95]<br></td>
                                <td class="tg-c3ow">large</td>
                                <td class="tg-c3ow">100</td>
                                <td class="tg-c3ow">0.326<br></td>
                              </tr>
                              <tr>
                                <td class="tg-lboi" rowspan="6">AR</td>
                                <td class="tg-c3ow">[0.50 : 0.05 : 0.95]</td>
                                <td class="tg-c3ow">all</td>
                                <td class="tg-c3ow">1</td>
                                <td class="tg-c3ow">0.107</td>
                              </tr>
                              <tr>
                                <td class="tg-c3ow">[0.50 : 0.05 : 0.95]</td>
                                <td class="tg-c3ow">all</td>
                                <td class="tg-c3ow">10</td>
                                <td class="tg-c3ow">0.220</td>
                              </tr>
                              <tr>
                                <td class="tg-c3ow">[0.50 : 0.05 : 0.95]</td>
                                <td class="tg-c3ow">all</td>
                                <td class="tg-c3ow">100</td>
                                <td class="tg-c3ow">0.257</td>
                              </tr>
                              <tr>
                                <td class="tg-c3ow">[0.50 : 0.05 : 0.95]</td>
                                <td class="tg-c3ow">small</td>
                                <td class="tg-c3ow">100</td>
                                <td class="tg-c3ow">0.187</td>
                              </tr>
                              <tr>
                                <td class="tg-c3ow">[0.50 : 0.05 : 0.95]</td>
                                <td class="tg-c3ow">medium</td>
                                <td class="tg-c3ow">100</td>
                                <td class="tg-c3ow">0.467<br></td>
                              </tr>
                              <tr>
                                <td class="tg-c3ow">[0.50 : 0.05 : 0.95]</td>
                                <td class="tg-c3ow">large</td>
                                <td class="tg-c3ow">100</td>
                                <td class="tg-c3ow">0.511<br></td>
                              </tr>
                            </tbody>
                          </table>
                          <p class="describe-projects">
                            I also deployed this model on a Nvidia Jetson AGX Xavier device and you can check the result in the video below and the demo code is available in <a href="https://github.com/tmralmeida/tensorrt-yolov4" 
                            class="tooltip-test">tmralmeida</a>.
                          </p>
                          

                          <iframe width="100%" height="315" src="https://www.youtube.com/embed/63p3GyR8JPw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
          
                    </p>

                    <p class="describe-projects">
                      <b>You can check the repository at </b><a href="https://github.com/tmralmeida/bag-of-models/tree/master/CNNs/2-Object_Detection/scripts" class="tooltip-test">tmralmeida</a>.
                    </p>

                  </div>
                  <div class="modal-footer">
                    <button type="button" class="btn btn-primary" onclick="toggle_modals('#modal-detection', '#modal-classification')">Previous</button>
                    <button type="button" class="btn btn-primary" disabled>Next</button>
                    <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                  </div>
                </div>
              </div>
            </div>









            <!-- DATA MATRIX PROJECT -->
            <div class="card">
              <h2 class="card-header">Data Matrix Detection</h2>
              <div class="card-body">
                  <h5 class="card-title">Mar 2020</h5>
                  <p class="card-text">This work presents an implementation of a Faster R-CNN model to detect Data Matrix. This architecture
                  demonstrated quite accurate and consistent results by...</p>

                <button type="button" class="btn btn-primary" data-toggle="modal" data-target="#modal-faster">
                    Continue reading
                </button>
                <!-- Modal -->
                <div class="modal fade" id="modal-faster" tabindex="-1" role="dialog" aria-labelledby="faster-modal-title" aria-hidden="true">
                  <div class="modal-dialog modal-lg" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <h4 class="modal-title" id="faster-modal-title">faster-rcnn-data-matrix Project</h4>
                        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                          <span aria-hidden="true">&times;</span>
                        </button>
                      </div>
                      <div class="modal-body">
                          <p class="describe-projects">
                              This work presents an implementation of a Faster R-CNN model to detect Data Matrix. This architecture
                              demonstrated quite accurate and consistent results by detecting almost all landmarks throughout the test
                              set.
                          </p>
                          <p class="describe-projects">
                              It arose during my research work at University of Aveiro, Portugal. In this project, I went through
                              every step of training a deep neural network: data collection (images of this type of landmarks in different environments);
                              data labeling through the <a href="https://labelbox.com/" class="tooltip-test">Labelbox app</a>; then, the Faster R-CNN model 
                              was trained and evaluated through the <a href="https://github.com/facebookresearch/detectron2" class="tooltip-test">
                              Detectron2 platform</a>, which is a research platform that contains several state-of-the-art models such as Faster R-CNN, Mask R-CNN, 
                              RetinaNet, and DensePose ready to use. 
                          </p>
                          <p class="describe-projects">
                              <b>Advice:</b> For those who don't have much time to design the architecture, this kind of platforms is totally worth it.
                          </p>
                          <p class="describe-projects">
                            <h5 style="color:black">1. Dataset creation</h5>
                              <p class="describe-projects">
                                The dataset is one of the most important pieces of the overall Machine Learning solution, since each decision of the model is based on 
                                a previous training, which is performed on that data. Therefore, if the training procedure has been compromised, then the inference quality of the model will be worse.
                                Thus, in this stage of the work, we labeled correctly 156 training frames and 224 test images. This distribution of training/test sets
                                is not either the most common one or the most correct one. However, the number of class objects to detect is just one, and, although it
                                is a small patch of the image, it is a pretty distinguishable object from the rest of the image. So, the training set is equally composed of
                                two different environments: a common laboratory room with several objects spread around and a workshop with machinery. These choices allow to
                                obtain a more representative dataset. Regarding the test set, this is also equally distributed in two different enviroments: a hallway and a different
                                part of the workshop used in the training set.
                              </p>
                              <p class="describe-projects"> 
                                The two images below are 2 samples used in the training set. The left image represents an environment associated to a manufacturing facility and the 
                                right image represents a visually cluttered environment (many different objects) in a laboratory room.
                                <div class="row">
                                  <div class="col">
                                    <img class="img-responsive" src="images/projects/faster-data-matrix/train1.png" style="width: 100%;" alt="image 1 from train set">
                                  </div>
                                  <div class="col">
                                    <img class="img-responsive" src="images/projects/faster-data-matrix/train2.png" style="width: 100%;" alt="image 2 from train set">
                                  </div>
                                </div>
                              </p>
                              <p class="describe-projects">
                               Regarding the test set, two images are shown as examples: from a more visually neat environment (left image) to a more filled and cluttered one (
                                right image). 
                                <div class="row">
                                  <div class="col">
                                    <img class="img-responsive" src="images/projects/faster-data-matrix/test2.png" style="width: 100%" alt="image 1 from test set">
                                  </div>
                                  <div class="col">
                                    <img class="img-responsive" src="images/projects/faster-data-matrix/test1.png" style="width: 100%;" alt="image 2 from test set">
                                  </div>
                                  
                                </div>
                              </p>
                            </p>

                          <p class="describe-projects">
                            <h5 style="color:black">2. Faster R-CNN Training</h5>
                            <p class="describe-projects">
                              We decided to use this architecture because this type of deep neural networks is very performant (in comparison to other object detection architectures) when the 
                              objective is to detect small patches of the image. 
                              Moreover, the system where this neural network would be used (an Automated Guided Vehicle) does not move at high speeds, so the high-latency disadvantage of a proposal 
                              network would not be a problem in this application. 
                              It is also worth mentioning that when you are at the phase of choosing which Machine Learning approach to use, you have to take into account the practical
                              application where you are working at (Deep Learning is sometimes overkill for some applications, Machine Learning is much more than just Deep Learning).
                            </p>
                            <p class="describe-projects">
                              The training procedure of a deep neural network can be divided into 3 main steps: data loading, forward propagation and back propagation. The first step in this work 
                              implied to register our dataset in the dataset catalog of Detectron2. This is no more than a function that translates our dataset in a dictionary with certain fields.
                              You can check all these steps in my <a href="https://github.com/tmralmeida/faster-rcnn-data-matrix/blob/master/faster-rcnn-data-matrix.ipynb" class="tooltip-test">notebook</a>.
                              Finally, the second and third steps, Detectron2 makes everything by us, we just need to know how to use their API and choose some hyperparameters such as: batch size, 
                              learning rate, and the number of iterations.  
                            </p>
                          </p>
                          <p class="describe-projects">
                            <h5 style="color:black">3. Faster R-CNN Evaluation</h5>
                            <p class="describe-projects">
                              The evaluation of the model was also performed through the Detectron2 API. To do so, we evaluate our model trough the <a href="http://cocodataset.org/#detection-eval" 
                              class="tooltip-test">COCO metrics</a> (the figure below shows our results).
                            </p>
                            
                            <img class="img-responsive" src="images/projects/faster-data-matrix/results.png" style="width: 60%" alt="results test set">
                            
                            <p class="describe-projects">
                              The most important overall result is 0.876 for AP@0.5. Why? Because 0.5 is a fair value for the IoU threshold, the scales are all and the number maximum of 
                              detections is 100 (a suitable value to match the reality). Moreover, the recall is higher than the precision, implying that the number of false positives is 
                              higher than the number of false negatives. This means that the model detects almost all the Data Matrix landmarks, but also detects some other objects that are not. In our
                              system this is preferable since we use a Data Matrix decoder in a further step. So, if the detected object is not a Data Matrix, the decoder would return nothing. 
                              Comparing to the Data Matrix detection provided by <a href="https://pypi.org/project/pylibdmtx/" class="tooltip-test">libdmtx Python library</a>, only 45% of the test 
                              set frames were accurately processed by this classical algorithm, being 40 times slower than the model that we trained in this project. 
                            </p>
                            <p class="describe-projects">
                              Finally, we show a video that demonstrate part of the qualitative results of the test set. The results shown here are not at the normal speed due to the video size 
                              (this is 1fps and our model can achieve 7.4fps).  
                            </p>                            
                              <iframe width="100%" height="315" src="https://www.youtube.com/embed/41Ul3dbbNDw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                         
                            <p class="describe-projects">
                              <b>You can check the repository at </b><a href="https://github.com/tmralmeida/faster-rcnn-data-matrix" class="tooltip-test">tmralmeida</a>.
                            </p>
                          </p>
                        </div>
                      <div class="modal-footer">
                        <button type="button" class="btn btn-primary" disabled>Previous</button>
                        <button type="button" class="btn btn-primary" onclick="toggle_modals('#modal-faster,#modal-bag')">Next</button>
                        <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            

        </main>

        <footer class="mastfoot mt-auto">
            <div class="inner">
                <p>last update: 08/10/2025</p>
            </div>
        </footer>
    </div>


</body>

</html>